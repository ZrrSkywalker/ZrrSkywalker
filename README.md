### Hi there ðŸ‘‹

ðŸŒ± I'm [Renrui Zhang](https://zrrskywalker.github.io/), a Ph.D. candidate in MMLab, CUHK.

#### Education
* [2017-2021] ðŸŽ‰ I received my B.E. degree from [Peking University](https://english.pku.edu.cn/), awarded *Outstanding Graduates* (top 5\%).
* [2020-2021] I worked as a visiting student in University of Pennsylvania, supervised by [Prof. Jianbo Shi](https://scholar.google.com/citations?user=Sm14jYIAAAAJ&hl=zh-CN&oi=ao).
* [2021-Now] ðŸ’ª I'm pursing my Ph.D. in [MMLab](https://mmlab.ie.cuhk.edu.hk/people.html), [CUHK](https://www.cuhk.edu.hk/english/index.html), supervised by [Prof. Hongsheng Li](https://www.ee.cuhk.edu.hk/~hsli/) and [Prof. Xiaogang Wang](https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=zh-CN).
* [2021-Now] I'm working as a research intern at Shanghai AI Lab, supervised by [Dr. Peng Gao](https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=zh-CN).

#### Research Projects
* Multi-modality Alignment: [ImageBind-LLM](https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/imagebind_LLM) and [Point-Bind](https://github.com/ZrrSkywalker/Point-Bind)
* Personalization of Segment Anything: [PerSAM and PerSAM-F](https://github.com/ZrrSkywalker/Personalize-SAM)
* Instruction Tuning of LLaMA: [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter) -> [LLaMA-Adapter V2](https://github.com/ZrrSkywalker/LLaMA-Adapter)
* Efficient Adaption of CLIP in 2D: [CLIP-Adapter](https://github.com/gaopengcuhk/CLIP-Adapter) -> [Tip-Adapter](https://github.com/gaopengcuhk/Tip-Adapter) -> [CaFo](https://github.com/ZrrSkywalker/CaFo), [APE](https://github.com/yangyangyang127/APE)
* MAE for 3D Point Clouds: [Point-M2AE](https://github.com/ZrrSkywalker/Point-M2AE) -> [I2P-MAE](https://github.com/ZrrSkywalker/I2P-MAE)
* Cross-modal Adaption of CLIP in 3D: [PointCLIP](https://github.com/ZrrSkywalker/PointCLIP) -> [PointCLIP V2](https://github.com/yangyangyang127/PointCLIP_V2)
* Non-Parametric 3D Analysis: [Point-NN and Point-PN](https://github.com/ZrrSkywalker/Point-NN)
* Camera-based 3D Object Detection: [MonoDETR](https://github.com/ZrrSkywalker/MonoDETR), [MonoDETR-MV](https://github.com/ZrrSkywalker/MonoDETR-MV) -> [TiG-BEV](https://github.com/ADLab3Ds/TiG-BEV)

#### News
* [2023-06-05] Release the code of [Point-Bind](https://github.com/ZrrSkywalker/Point-Bind) for aligning different modalities with 3D point clouds.
* [2023-05-29] Release the code of [ImageBind-LLM](https://github.com/ZrrSkywalker/LLaMA-Adapter/tree/main/imagebind_LLM) for multi-modality instruction tuning of LLaMA.
* [2023-05-05] Release the paper [PerSAM](https://github.com/ZrrSkywalker/Personalize-SAM/blob/main/paper_arXiv.pdf) and [code](https://github.com/ZrrSkywalker/Personalize-SAM) for personalizing Segment Anything within 10 seconds.
* [2023-04-29] Release the paper [LLaMA-Adapter V2](https://github.com/ZrrSkywalker/LLaMA-Adapter/blob/main/LLaMA-Adapter-V2-arXiv.pdf) and [code](https://github.com/ZrrSkywalker/LLaMA-Adapter/tree/main/llama_adapter_v2_chat65b) for stronger multi-modal reasoning.
* [2023-04-16] Release the training code of [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter).
* [2023-04-11] Release the code of [Point-PN](https://github.com/ZrrSkywalker/Point-NN).
* [2023-04-03] Release the code of [Point-M2AE](https://github.com/ZrrSkywalker/Point-M2AE) and [I2P-MAE](https://github.com/ZrrSkywalker/I2P-MAE).
* [2023-04-01] Release [CaFo](https://github.com/ZrrSkywalker/CaFo) cascaded with ChatGPT and Stable Diffusion.

<!--
**ZrrSkywalker/ZrrSkywalker** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
